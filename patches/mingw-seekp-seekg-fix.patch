diff --git a/llama.cpp b/llama.cpp
index 6e23a077..1825b5ee 100644
--- a/llama.cpp
+++ b/llama.cpp
@@ -5868,7 +6023,11 @@ static void llama_model_quantize_internal(const std::string & fname_inp, const s
 
     // go back to beginning of file and write the updated meta data
     {
+#ifdef __MINGW32__
+        fout.seekp(0, std::ios::beg);
+#else
         fout.seekp(0);
+#endif
         std::vector<uint8_t> data(gguf_get_meta_size(ctx_out));
         gguf_get_meta_data(ctx_out, data.data());
         fout.write((const char *) data.data(), data.size());
@@ -6050,7 +6209,11 @@ static int llama_apply_lora_from_file_internal(
         size_t offset = fin.tellg();
         size_t tensor_data_size = ggml_nbytes(lora_tensor);
         offset = (offset + 31) & -32;
+#ifdef __MINGW32__
+        fin.seekg(static_cast<std::streamoff>(offset), std::ios::beg);
+#else
         fin.seekg(offset);
+#endif
         fin.read((char*)lora_tensor->data, tensor_data_size);
 
         lora_tensors[name] = lora_tensor;